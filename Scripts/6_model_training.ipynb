{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.960029Z",
     "start_time": "2020-12-29T17:03:34.298Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import t5\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import T5Tokenizer, MT5ForConditionalGeneration, T5ForConditionalGeneration, Adafactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.960647Z",
     "start_time": "2020-12-29T17:03:34.332Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data...\n",
    "\n",
    "datasets = ['long', 'long_fully_aligned', 'long_softly_aligned',\n",
    "            'short', 'short_fully_aligned', 'short_softly_aligned']\n",
    "dataset_version = datasets[4]\n",
    "\n",
    "\n",
    "# Model...\n",
    "\n",
    "models = ['t5-small', 't5-base', 't5-large', 'google/mt5-small', 'google/mt5-base']\n",
    "model_version = models[0]\n",
    "\n",
    "\n",
    "# Spa encoding...\n",
    "\n",
    "spa_char_encode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.961287Z",
     "start_time": "2020-12-29T17:03:34.371Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open('../Datasets/wikimusica_'+dataset_version+'.p', \"rb\"))\n",
    "\n",
    "input_train = data[0]\n",
    "input_test = data[1]\n",
    "output_train = data[2]\n",
    "output_test = data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode spanish characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.961922Z",
     "start_time": "2020-12-29T17:03:34.406Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_unseen_characters(text: str):\n",
    "    \n",
    "    text = (text.replace('í','%i%')\n",
    "                .replace('Í','%I%')\n",
    "                .replace('ú','%u%')\n",
    "                .replace('Ú','%U%')\n",
    "                .replace('Á','%A%')\n",
    "                .replace('Ó','%O%')\n",
    "                .replace('ñ','%n%')\n",
    "                .replace('Ñ','%N%'))\n",
    "    \n",
    "    return text\n",
    "\n",
    "#### #### ####\n",
    "\n",
    "def decode_unseen_characters(text: str):\n",
    "    \n",
    "    text = (text.replace('%i%','í')\n",
    "                .replace('%I%','Í')\n",
    "                .replace('%u%','ú')\n",
    "                .replace('%U%','Ú')\n",
    "                .replace('%A%','Á')\n",
    "                .replace('%O%','Ó')\n",
    "                .replace('%n%','ñ')\n",
    "                .replace('%N%','Ñ'))\n",
    "    \n",
    "    return text   \n",
    "\n",
    "#### #### ####\n",
    "\n",
    "if spa_char_encode:\n",
    "    input_train = [encode_unseen_characters(d) for d in input_train]\n",
    "    input_test = [encode_unseen_characters(d) for d in input_test]\n",
    "    output_train = [encode_unseen_characters(d) for d in output_train]\n",
    "    output_test = [encode_unseen_characters(d) for d in output_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.962533Z",
     "start_time": "2020-12-29T17:03:34.441Z"
    }
   },
   "outputs": [],
   "source": [
    "input_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.963144Z",
     "start_time": "2020-12-29T17:03:34.443Z"
    }
   },
   "outputs": [],
   "source": [
    "output_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.963724Z",
     "start_time": "2020-12-29T17:03:34.444Z"
    }
   },
   "outputs": [],
   "source": [
    "input_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.964292Z",
     "start_time": "2020-12-29T17:03:34.446Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.965019Z",
     "start_time": "2020-12-29T17:03:34.484Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.965662Z",
     "start_time": "2020-12-29T17:03:34.486Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_token_lengths(tokenizer, input_train, output_train, input_test, output_test):\n",
    "    \n",
    "    check_l = [input_train, input_test, output_train, output_test]\n",
    "    names_l = ['input train', 'input test', 'output train', 'output test']\n",
    "    \n",
    "    for i,c in enumerate(check_l):\n",
    "        \n",
    "        length = [len(tokenizer.tokenize(i)) for i in c]\n",
    "        print('Max '+names_l[i]+' token length: ',max(length))\n",
    "        if i==1:\n",
    "            print('-- --')\n",
    "\n",
    "####\n",
    "\n",
    "check_token_lengths(tokenizer, input_train, output_train, input_test, output_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.966295Z",
     "start_time": "2020-12-29T17:03:34.519Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_val_loss_points(input_train, divisor):\n",
    "    \n",
    "    len_input = math.ceil(len(input_train)/divisor)\n",
    "    for i in range(120,600):\n",
    "        if len_input%i==0:\n",
    "            print('batch divisor: ',i)\n",
    "            return i\n",
    "        \n",
    "####\n",
    "divisor=2\n",
    "batch_divisor = calculate_val_loss_points(input_train, divisor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.966923Z",
     "start_time": "2020-12-29T17:03:34.554Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Start a W&B run\n",
    "wandb.init(project='NLG WikiMusica')\n",
    "run_id = wandb.run.id\n",
    "\n",
    "# 2. Save model inputs and hyperparameters\n",
    "config = wandb.config\n",
    "\n",
    "config.TRAIN_BATCH_SIZE = 8\n",
    "config.VALID_BATCH_SIZE = 2\n",
    "config.EPOCHS = 12\n",
    "\n",
    "config.SEED = 89     \n",
    "\n",
    "config.MAX_INPUT_LEN = 400\n",
    "config.MAX_OUTPUT_LEN = 270\n",
    "\n",
    "config.LEARNING_RATE = 1e-3\n",
    "config.EPS = (1e-30, 1e-3)\n",
    "config.CLIP_THRESHOLD = 1.0\n",
    "config.DECAY_RATE = -0.8\n",
    "config.BETA1 = None\n",
    "config.WEIGHT_DECAY = 0.0\n",
    "config.RELATIVE_STEP = False\n",
    "config.SCALE_PARAMETER = False\n",
    "config.WARMUP_INIT = False\n",
    "\n",
    "####\n",
    "\n",
    "config.DATASET = dataset_version\n",
    "config.MODEL = model_version\n",
    "config.ENCODE_SPA_CHAR = spa_char_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.967581Z",
     "start_time": "2020-12-29T17:03:34.555Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.968232Z",
     "start_time": "2020-12-29T17:03:34.557Z"
    }
   },
   "outputs": [],
   "source": [
    "class NLGWikiDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_input, data_output, tokenizer, source_len, target_len):\n",
    "        \n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_len = source_len\n",
    "        self.target_len = target_len\n",
    "        self.attr = data_input\n",
    "        self.target = data_output\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.attr)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        attr = self.attr[index]\n",
    "        target = self.target[index]\n",
    "        \n",
    "        source = self.tokenizer.batch_encode_plus([attr], pad_to_max_length=True, max_length=self.source_len,  return_tensors='pt')\n",
    "        target = self.tokenizer.batch_encode_plus([target], pad_to_max_length=True, max_length=self.target_len, return_tensors='pt')    \n",
    "        \n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_mask = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'source_ids': source_ids.to(dtype=torch.long), \n",
    "            'source_mask': source_mask.to(dtype=torch.long), \n",
    "            'target_ids': target_ids.to(dtype=torch.long),\n",
    "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.968883Z",
     "start_time": "2020-12-29T17:03:34.559Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer, batch_divisor):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch=epoch+1\n",
    "    log_chunk = round(len(loader)/batch_divisor)\n",
    "    log_step = 1\n",
    "    loss_sum = 0\n",
    "    \n",
    "    total_epoch_loss = 0\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        \n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "        input_ids = data['source_ids'].to(device)\n",
    "        labels = data['target_ids'].to(device)\n",
    "          \n",
    "        # clear out the gradients of all Variables \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids = input_ids, attention_mask = mask, labels=labels)\n",
    "        \n",
    "        loss = outputs[0]  ## outputs.loss\n",
    "        loss_sum += loss.item()\n",
    "        total_epoch_loss += loss.item()\n",
    "        \n",
    "        if (_+1)%log_chunk == 0:\n",
    "            \n",
    "            loss_mean = (loss_sum/log_chunk)\n",
    "            x_axis = round((log_step+(epoch-1)*batch_divisor)/batch_divisor,3)\n",
    "            print(f'{x_axis}. Epoch: {epoch}, Training Loss:  {loss_mean}')\n",
    "            wandb.log({\"Training Loss\": loss_mean, \"epoch\": x_axis, \"epoch_n\": epoch})\n",
    "            log_step += 1\n",
    "            loss_sum = 0\n",
    "\n",
    "        # calculating the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # updating the params\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Save full epoch loss\n",
    "    full_epoch_loss = total_epoch_loss/len(loader)\n",
    "    wandb.log({\"Training Epoch Loss\": full_epoch_loss, \"epoch_n\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.969551Z",
     "start_time": "2020-12-29T17:03:34.561Z"
    }
   },
   "outputs": [],
   "source": [
    "def validate(epoch, tokenizer, model, device, loader, batch_divisor):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch=epoch+1\n",
    "    log_chunk = round(len(loader)/batch_divisor)\n",
    "    log_step = 1\n",
    "    loss_sum = 0\n",
    "    \n",
    "    total_epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for _,data in enumerate(loader, 0):\n",
    "\n",
    "            input_ids = data['source_ids'].to(device)\n",
    "            labels = data['target_ids'].to(device)\n",
    "\n",
    "            outputs = model(input_ids = input_ids, labels=labels)\n",
    "            \n",
    "            loss = outputs[0]  ## outputs.loss\n",
    "            loss_sum += loss.item()\n",
    "            total_epoch_loss += loss.item()\n",
    "\n",
    "            if (_+1)%log_chunk == 0:\n",
    "            \n",
    "                loss_mean = (loss_sum/log_chunk)\n",
    "                x_axis = round((log_step+(epoch-1)*batch_divisor)/batch_divisor,3)\n",
    "                print(f'{x_axis}. Epoch: {epoch}, Validation Loss:  {loss_mean}')\n",
    "                wandb.log({\"Validation Loss\": loss_mean, \"epoch\": x_axis})\n",
    "                log_step += 1\n",
    "                loss_sum = 0\n",
    "                \n",
    "        # Save full epoch loss\n",
    "        full_epoch_loss = total_epoch_loss/len(loader)\n",
    "        wandb.log({\"Validation Epoch Loss\": full_epoch_loss, \"epoch_n\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.970191Z",
     "start_time": "2020-12-29T17:03:34.563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set random seeds and deterministic pytorch for reproducibility\n",
    "\n",
    "torch.manual_seed(config.SEED)            # pytorch random seed\n",
    "np.random.seed(config.SEED)               # numpy random seed\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.971491Z",
     "start_time": "2020-12-29T17:03:34.566Z"
    }
   },
   "outputs": [],
   "source": [
    "training_set = NLGWikiDataset(input_train,\n",
    "                              output_train,\n",
    "                              tokenizer,\n",
    "                              config.MAX_INPUT_LEN,\n",
    "                              config.MAX_OUTPUT_LEN)\n",
    "\n",
    "validation_set = NLGWikiDataset(input_test,\n",
    "                                output_test,\n",
    "                                tokenizer,\n",
    "                                config.MAX_INPUT_LEN,\n",
    "                                config.MAX_OUTPUT_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.972098Z",
     "start_time": "2020-12-29T17:03:34.568Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the parameters for creation of dataloaders\n",
    "train_params = {\n",
    "    'batch_size': config.TRAIN_BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "\n",
    "val_params = {\n",
    "    'batch_size': config.VALID_BATCH_SIZE,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "    }\n",
    "\n",
    "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "validation_loader = DataLoader(validation_set, **val_params)\n",
    "\n",
    "print('-- * -- * -- * --')\n",
    "\n",
    "print(f'Batch Size (Train): {config.TRAIN_BATCH_SIZE}')\n",
    "print(f'Batch Size (Validation): {config.VALID_BATCH_SIZE}')\n",
    "\n",
    "print(f'Length Loader (Train): {len(training_loader)}')\n",
    "print(f'Length Loader (Validation): {len(validation_loader)}')\n",
    "\n",
    "print(f'Batch Divisor (Train): {len(training_loader)/batch_divisor}')\n",
    "print(f'Batch Divisor (Validation): {len(validation_loader)/batch_divisor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.972695Z",
     "start_time": "2020-12-29T17:03:34.570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "model = T5ForConditionalGeneration.from_pretrained(config.MODEL)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T21:58:17.973307Z",
     "start_time": "2020-12-29T17:03:34.572Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "optimizer = Adafactor(params =  model.parameters(), \n",
    "                      lr = config.LEARNING_RATE,\n",
    "                      eps = config.EPS,\n",
    "                      clip_threshold = config.CLIP_THRESHOLD,\n",
    "                      decay_rate = config.DECAY_RATE,\n",
    "                      beta1 = config.BETA1,\n",
    "                      weight_decay = config.WEIGHT_DECAY,\n",
    "                      relative_step = config.RELATIVE_STEP,\n",
    "                      scale_parameter = config.SCALE_PARAMETER,\n",
    "                      warmup_init = config.WARMUP_INIT)\n",
    "\n",
    "\n",
    "\n",
    "# Training & Validation loop\n",
    "print('Initiating Fine-Tuning for the model on our dataset')\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "for epoch in range(config.EPOCHS):\n",
    "    \n",
    "    if epoch!=0:\n",
    "        wandb.init(project='NLG WikiMusica', resume=run_id)\n",
    "    \n",
    "    # Log metrics with wandb\n",
    "    train(epoch, tokenizer, model, device, training_loader, optimizer, batch_divisor)\n",
    "    validate(epoch, tokenizer, model, device, validation_loader, batch_divisor)\n",
    "    \n",
    "    try:\n",
    "        # Save model for each epoch\n",
    "        wandb.unwatch()\n",
    "        torch.save(model, f'../Models/{wandb.run.name}-{epoch+1}.pt')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
