{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:49:15.583299Z",
     "start_time": "2020-12-30T01:49:12.708821Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import t5\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import T5Tokenizer, MT5ForConditionalGeneration, T5ForConditionalGeneration, Adafactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:49:15.588453Z",
     "start_time": "2020-12-30T01:49:15.584922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data...\n",
    "\n",
    "datasets = ['long', 'long_fully_aligned', 'long_softly_aligned',\n",
    "            'short', 'short_fully_aligned', 'short_softly_aligned']\n",
    "\n",
    "wa = datasets[0] # without_alignment\n",
    "ls = datasets[1] # long_strict\n",
    "lr = datasets[2] # long_relaxed\n",
    "ss = datasets[4] # short_strict\n",
    "sr = datasets[5] # short_relaxed\n",
    "\n",
    "# Model...\n",
    "\n",
    "models = ['t5-small', 't5-base', 't5-large', 'google/mt5-small', 'google/mt5-base']\n",
    "\n",
    "model_version = models[0]\n",
    "\n",
    "\n",
    "# Encoding...\n",
    "\n",
    "spa_char_encode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:49:15.681305Z",
     "start_time": "2020-12-30T01:49:15.590239Z"
    }
   },
   "outputs": [],
   "source": [
    "data_wa = pickle.load(open('../Datasets/wikimusica_'+wa+'.p', \"rb\"))\n",
    "input_test_wa = data_wa[1]\n",
    "output_test_wa = data_wa[3]\n",
    "\n",
    "data_ls = pickle.load(open('../Datasets/wikimusica_'+ls+'.p', \"rb\"))\n",
    "input_test_ls = data_ls[1]\n",
    "output_test_ls = data_ls[3]\n",
    "\n",
    "data_lr = pickle.load(open('../Datasets/wikimusica_'+lr+'.p', \"rb\"))\n",
    "input_test_lr = data_lr[1]\n",
    "output_test_lr = data_lr[3]\n",
    "\n",
    "data_ss = pickle.load(open('../Datasets/wikimusica_'+ss+'.p', \"rb\"))\n",
    "input_test_ss = data_ss[1]\n",
    "output_test_ss = data_ss[3]\n",
    "\n",
    "data_sr = pickle.load(open('../Datasets/wikimusica_'+sr+'.p', \"rb\"))\n",
    "input_test_sr = data_sr[1]\n",
    "output_test_sr = data_sr[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select samples for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:49:15.694043Z",
     "start_time": "2020-12-30T01:49:15.682815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select samples after manual checking\n",
    "\n",
    "ss_ind = [0,   2,   3,   5,   7, \n",
    "          8,   10,  12,  14,  15,\n",
    "          16,  18,  21,  22,  23,\n",
    "          29,  30,  31,  32,  34,]\n",
    "\n",
    "ls_ind = [35,  36,  37,  38,  41,\n",
    "          42,  43,  44,  45,  46,\n",
    "          47,  48,  49,  50,  51,\n",
    "          52,  53,  55,  57,  59,\n",
    "          60,  61,  62,  63,  66,\n",
    "          70,  72,  76,  77,  80,]\n",
    "              \n",
    "sr_ind = [81,  82,  84,  85,  86,\n",
    "          89,  90,  95,  96,  97,\n",
    "          99,  100, 101, 102, 105,\n",
    "          106, 107, 108, 109, 110,\n",
    "          111, 112, 113, 114, 116,\n",
    "          117, 118, 119, 120, 125]\n",
    "              \n",
    "lr_ind = [127, 129, 130, 131, 133,\n",
    "          136, 137, 140, 142, 144,\n",
    "          145, 146, 147, 148, 149,\n",
    "          150, 151, 152, 153, 154, \n",
    "          158, 159, 160, 162, 153,\n",
    "          166, 167, 168, 170, 173]\n",
    "\n",
    "wa_ind = [175, 178, 179, 180, 182,\n",
    "         184, 185, 187, 188, 190,\n",
    "         191, 193, 196, 198, 200,\n",
    "         201, 202, 203, 205, 206,\n",
    "         207, 217, 224, 225, 228,\n",
    "         229, 234, 240, 241, 242,       \n",
    "         259, 277, 289, 312, 314,\n",
    "         328, 341, 368, 374, 395,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:49:15.708295Z",
     "start_time": "2020-12-30T01:49:15.695384Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_test = list()\n",
    "output_ref_test = list()\n",
    "\n",
    "[input_test.append(input_test_ss[d]) for d in ss_ind]\n",
    "[input_test.append(input_test_ls[d]) for d in ls_ind]\n",
    "[input_test.append(input_test_sr[d]) for d in sr_ind]\n",
    "[input_test.append(input_test_lr[d]) for d in lr_ind]\n",
    "[input_test.append(input_test_wa[d]) for d in wa_ind]\n",
    "\n",
    "[output_ref_test.append(output_test_ss[d]) for d in ss_ind]\n",
    "[output_ref_test.append(output_test_ls[d]) for d in ls_ind]\n",
    "[output_ref_test.append(output_test_sr[d]) for d in sr_ind]\n",
    "[output_ref_test.append(output_test_lr[d]) for d in lr_ind]\n",
    "[output_ref_test.append(output_test_wa[d]) for d in wa_ind]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop some selected samples, (we want just 100)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:49:15.719839Z",
     "start_time": "2020-12-30T01:49:15.715425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, instead of 150 samples, we will test over 100\n",
    "\n",
    "to_drop = [14, 21, 26, 34, 35, 42, 48, 49, 51, 54,\n",
    "           59, 65, 67, 68, 70, 80, 81, 84, 95, 96,\n",
    "           97, 102, 103, 23, 25, 29, 31, 38, 39, 53,\n",
    "           147, 145, 141, 140, 137, 135, 130, 129, 127, 125,\n",
    "           119, 115, 113, 111, 57, 66, 72, 100, 109, 134]\n",
    "\n",
    "\n",
    "new_input_test = list()\n",
    "new_output_ref_test = list()\n",
    "for i in range(150):\n",
    "    if i not in to_drop:\n",
    "        new_input_test.append(input_test[i])\n",
    "        new_output_ref_test.append(output_ref_test[i])\n",
    "    \n",
    "input_test = new_input_test\n",
    "output_ref_test = new_output_ref_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check attribute count for selected samples** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:49:16.089453Z",
     "start_time": "2020-12-30T01:49:15.721351Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_attr_num = list()\n",
    "input_attr = list()\n",
    "\n",
    "for i in range(150):\n",
    "    print(i)\n",
    "    attr = {}\n",
    "    attr_num = {'stagename':0,\n",
    "                'birthname':0,\n",
    "\n",
    "                'birthplace':0,\n",
    "                'nation':0,\n",
    "                'birthdate':0,\n",
    "                'deathplace':0,\n",
    "                'deathdate':0,\n",
    "\n",
    "                'occupation':0,\n",
    "                'instrument':0,\n",
    "                'voice':0,\n",
    "                'genre':0,\n",
    "                'group':0,}\n",
    "    \n",
    "        \n",
    "    for n in input_test[i].split('wikimusic: ')[1].split(' • '):\n",
    "        a = n.split(' | ')[0]\n",
    "        b = n.split(' | ')[1]\n",
    "        \n",
    "        attr_num[a] += 1\n",
    "        try:\n",
    "            attr[a].append(b)\n",
    "        except:\n",
    "            attr[a] = [b]\n",
    "            \n",
    "        print(n)\n",
    "        \n",
    "    input_attr_num.append(attr_num)\n",
    "    input_attr.append(attr)\n",
    "    \n",
    "    print('----')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:49:16.090873Z",
     "start_time": "2020-12-30T01:49:15.761Z"
    }
   },
   "outputs": [],
   "source": [
    "df_attr_num = pd.DataFrame(input_attr_num)\n",
    "df_attr_num.groupby('group').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:49:17.320156Z",
     "start_time": "2020-12-30T01:49:17.316363Z"
    }
   },
   "outputs": [],
   "source": [
    "input_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare input data for human evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:49:19.476037Z",
     "start_time": "2020-12-30T01:49:19.466734Z"
    }
   },
   "outputs": [],
   "source": [
    "name = ['stagename', 'birthname']\n",
    "birth = ['birthplace', 'nation', 'birthdate', 'deathplace', 'deathdate']\n",
    "info = ['occupation', 'instrument', 'voice', 'genre', 'group']\n",
    "\n",
    "input_1_name = list()\n",
    "input_1_birth = list()\n",
    "input_1_info = list()\n",
    "\n",
    "for i in input_attr:\n",
    "    text_name = ''\n",
    "    text_birth = ''\n",
    "    text_info = ''\n",
    "    \n",
    "    for k in i.keys():\n",
    "        for e in i[k]:\n",
    "            if k in name:\n",
    "                text_name += k + ': ' + e + '\\n'\n",
    "            elif k in birth:\n",
    "                text_birth += k + ': ' + e + '\\n'\n",
    "            else:\n",
    "                text_info += k + ': ' + e + '\\n'\n",
    "            \n",
    "    input_1_name.append(text_name)\n",
    "    input_1_birth.append(text_birth)\n",
    "    input_1_info.append(text_info)\n",
    "    \n",
    "    \n",
    "######\n",
    "\n",
    "\n",
    "input_final = list()\n",
    "for i in range(100):\n",
    "    input_final.append(input_1_name[i] + '-----\\n' + input_1_birth[i] + '-----\\n' + input_1_info[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Dataframe for evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_records = list()\n",
    "for i in range(100):\n",
    "    record = {\n",
    "        'reference': output_ref_test[i],\n",
    "        'input_for_model': input_test[i],\n",
    "        'attributes': input_final[i],\n",
    "    }\n",
    "    test_records.append(record)\n",
    "    \n",
    "df_test = pd.DataFrame(test_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add unique id-s for tracing each generated text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:49:21.544525Z",
     "start_time": "2020-12-30T01:49:21.514814Z"
    }
   },
   "outputs": [],
   "source": [
    "ids = list(range(1,1101))\n",
    "\n",
    "df_test['text_model_v1'] = ''\n",
    "df_test['text_model_v1_id'] = random.sample(ids, k=df_test.shape[0])\n",
    "ids_to_remove = df_test['text_model_v1_id'].tolist()\n",
    "_ = [ids.remove(i) for i in ids_to_remove]\n",
    "\n",
    "df_test['text_model_v2'] = ''\n",
    "df_test['text_model_v2_id'] = random.sample(ids, k=df_test.shape[0])\n",
    "ids_to_remove = df_test['text_model_v2_id'].tolist()\n",
    "_ = [ids.remove(i) for i in ids_to_remove]\n",
    "\n",
    "df_test['text_model_v3'] = ''\n",
    "df_test['text_model_v3_id'] = random.sample(ids, k=df_test.shape[0])\n",
    "ids_to_remove = df_test['text_model_v3_id'].tolist()\n",
    "_ = [ids.remove(i) for i in ids_to_remove]\n",
    "\n",
    "df_test['text_model_v4'] = ''\n",
    "df_test['text_model_v4_id'] = random.sample(ids, k=df_test.shape[0])\n",
    "ids_to_remove = df_test['text_model_v4_id'].tolist()\n",
    "_ = [ids.remove(i) for i in ids_to_remove]\n",
    "\n",
    "df_test['text_model_v5'] = ''\n",
    "df_test['text_model_v5_id'] = random.sample(ids, k=df_test.shape[0])\n",
    "ids_to_remove = df_test['text_model_v5_id'].tolist()\n",
    "_ = [ids.remove(i) for i in ids_to_remove]\n",
    "\n",
    "df_test['text_model_v6'] = ''\n",
    "df_test['text_model_v6_id'] = random.sample(ids, k=df_test.shape[0])\n",
    "ids_to_remove = df_test['text_model_v6_id'].tolist()\n",
    "_ = [ids.remove(i) for i in ids_to_remove]\n",
    "\n",
    "df_test['text_model_v7'] = ''\n",
    "df_test['text_model_v7_id'] = random.sample(ids, k=df_test.shape[0])\n",
    "ids_to_remove = df_test['text_model_v7_id'].tolist()\n",
    "_ = [ids.remove(i) for i in ids_to_remove]\n",
    "\n",
    "df_test['text_model_v8'] = ''\n",
    "df_test['text_model_v8_id'] = random.sample(ids, k=df_test.shape[0])\n",
    "ids_to_remove = df_test['text_model_v8_id'].tolist()\n",
    "_ = [ids.remove(i) for i in ids_to_remove]\n",
    "\n",
    "df_test['text_model_v9'] = ''\n",
    "df_test['text_model_v9_id'] = random.sample(ids, k=df_test.shape[0])\n",
    "ids_to_remove = df_test['text_model_v9_id'].tolist()\n",
    "_ = [ids.remove(i) for i in ids_to_remove]\n",
    "\n",
    "df_test['text_model_v10'] = ''\n",
    "df_test['text_model_v10_id'] = random.sample(ids, k=df_test.shape[0])\n",
    "ids_to_remove = df_test['text_model_v10_id'].tolist()\n",
    "_ = [ids.remove(i) for i in ids_to_remove]\n",
    "\n",
    "df_test['text_model_v11'] = df_test['reference']\n",
    "df_test['text_model_v11_id'] = random.sample(ids, k=df_test.shape[0])\n",
    "ids_to_remove = df_test['text_model_v11_id'].tolist()\n",
    "_ = [ids.remove(i) for i in ids_to_remove]\n",
    "\n",
    "\n",
    "# Shuffle data\n",
    "df_test = df_test.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate texts for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T23:30:27.992821Z",
     "start_time": "2020-12-29T23:30:27.985193Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_unseen_characters(text: str):\n",
    "    \n",
    "    text = (text.replace('í','%i%')\n",
    "                .replace('Í','%I%')\n",
    "                .replace('ú','%u%')\n",
    "                .replace('Ú','%U%')\n",
    "                .replace('Á','%A%')\n",
    "                .replace('Ó','%O%')\n",
    "                .replace('ñ','%n%')\n",
    "                .replace('Ñ','%N%'))\n",
    "    \n",
    "    return text\n",
    "\n",
    "def decode_unseen_characters(text: str):\n",
    "    \n",
    "    text = (text.replace('%i%','í')\n",
    "                .replace('%I%','Í')\n",
    "                .replace('%u%','ú')\n",
    "                .replace('%U%','Ú')\n",
    "                .replace('%A%','Á')\n",
    "                .replace('%O%','Ó')\n",
    "                .replace('%n%','ñ')\n",
    "                .replace('%N%','Ñ'))\n",
    "    \n",
    "    return text   \n",
    "\n",
    "#### #### ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T23:30:28.397078Z",
     "start_time": "2020-12-29T23:30:28.394458Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T23:30:31.282956Z",
     "start_time": "2020-12-29T23:30:28.907020Z"
    }
   },
   "outputs": [],
   "source": [
    "model_version = 't5-base'\n",
    "t5_base_tokenizer = T5Tokenizer.from_pretrained(model_version)\n",
    "\n",
    "####################\n",
    "\n",
    "model_name = 'jumping-jazz-13-3.pt' # t-5 vanila (long)\n",
    "\n",
    "v1_vanila = torch.load('../Models/'+model_name)\n",
    "v1_vanila.to(device);\n",
    "\n",
    "###\n",
    "\n",
    "model_name = 'classic-puddle-20-3.pt'\n",
    "\n",
    "v2_t5base_strict = torch.load('../Models/'+model_name)\n",
    "v2_t5base_strict.to(device);\n",
    "\n",
    "###\n",
    "\n",
    "#model_name = 'trim-firebrand-17-3.pt'\n",
    "model_name = 'vivid-darkness-18-3.pt'\n",
    "\n",
    "v6_t5base_relaxed = torch.load('../Models/'+model_name)\n",
    "v6_t5base_relaxed.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T23:30:32.165026Z",
     "start_time": "2020-12-29T23:30:31.284209Z"
    }
   },
   "outputs": [],
   "source": [
    "model_version = 't5-small'\n",
    "t5_small_tokenizer = T5Tokenizer.from_pretrained(model_version)\n",
    "\n",
    "####################\n",
    "\n",
    "model_name = 'kind-hill-15-2.pt'\n",
    "\n",
    "v7_t5small_strict = torch.load('../Models/'+model_name)\n",
    "v7_t5small_strict.to(device);\n",
    "\n",
    "###\n",
    "\n",
    "model_name = 'noble-oath-3-5.pt'\n",
    "\n",
    "v8_t5small_relaxed = torch.load('../Models/'+model_name)\n",
    "v8_t5small_relaxed.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T23:30:37.476865Z",
     "start_time": "2020-12-29T23:30:32.166534Z"
    }
   },
   "outputs": [],
   "source": [
    "model_version = 'google/mt5-small'\n",
    "mt5_small_tokenizer = T5Tokenizer.from_pretrained(model_version)\n",
    "\n",
    "####################\n",
    "\n",
    "model_name = 'peach-star-28-7.pt'\n",
    "\n",
    "v9_mt5small_strict = torch.load('../Models/'+model_name)\n",
    "v9_mt5small_strict.to(device);\n",
    "\n",
    "###\n",
    "\n",
    "model_name = 'zany-haze-27-4.pt'\n",
    "\n",
    "v10_mt5small_relaxed = torch.load('../Models/'+model_name)\n",
    "v10_mt5small_relaxed.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T23:30:37.481450Z",
     "start_time": "2020-12-29T23:30:37.478368Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_occupation(text:str):\n",
    "    \n",
    "    ''' Function for checking if any occupation is present in input data'''\n",
    "    \n",
    "    has_occ = text.find('occupation |')\n",
    "    if has_occ!=-1:\n",
    "        return text\n",
    "    else:\n",
    "        has_ins = text.find('instrument |')\n",
    "    if has_ins!=-1:\n",
    "        return text\n",
    "    else:\n",
    "        return text + ' • occupation | m%u%sico'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate eval text for each model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T23:30:37.493550Z",
     "start_time": "2020-12-29T23:30:37.482997Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(input_text, model, tokenizer,\n",
    "                  num_beams=10,\n",
    "                  min_length=30,\n",
    "                  num_return_sequences=1,\n",
    "                  length_penalty=1,\n",
    "                  no_repeat_ngram_size=0,\n",
    "                  dynamic_min_length=False,\n",
    "                  test=False):\n",
    "    \n",
    "    input_text = encode_unseen_characters(input_text)\n",
    "    \n",
    "    if dynamic_min_length:\n",
    "        attrs = input_text.split('•')\n",
    "        input_split = [a.split('|')[1].strip() for a in attrs]\n",
    "        input_split = ' '.join(input_split)\n",
    "        min_length = len(tokenizer.tokenize(input_split)) + 10\n",
    "\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "    \n",
    "    outputs = model.generate(input_ids=features['input_ids'],\n",
    "                             attention_mask=features['attention_mask'],\n",
    "                             max_length=512,\n",
    "                             min_length=min_length,\n",
    "                             num_beams=num_beams,\n",
    "                             num_return_sequences=num_return_sequences,\n",
    "                             length_penalty=length_penalty,\n",
    "                             no_repeat_ngram_size=no_repeat_ngram_size)\n",
    "\n",
    "    for output in outputs:\n",
    "        t = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        t = decode_unseen_characters(t)\n",
    "\n",
    "        if test:\n",
    "            print('\\n-- ** -- ** --\\n')\n",
    "            print(t)\n",
    "            \n",
    "        else:\n",
    "            return t            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T23:36:12.671168Z",
     "start_time": "2020-12-29T23:30:43.448566Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['text_model_v1'] = df_test['input_for_model'].progress_apply(generate_text,\n",
    "                                     model=v1_vanila,\n",
    "                                     tokenizer=t5_base_tokenizer,\n",
    "                                     num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T23:43:01.482283Z",
     "start_time": "2020-12-29T23:37:14.314498Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['text_model_v2'] = df_test['input_for_model'].progress_apply(generate_text,\n",
    "                                     model=v2_t5base_strict,\n",
    "                                     tokenizer=t5_base_tokenizer,\n",
    "                                     num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T23:46:36.384682Z",
     "start_time": "2020-12-29T23:43:01.483334Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['text_model_v3'] = df_test['input_for_model'].progress_apply(generate_text,\n",
    "                                     model=v2_t5base_strict,\n",
    "                                     tokenizer=t5_base_tokenizer,\n",
    "                                     num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T23:52:03.200617Z",
     "start_time": "2020-12-29T23:46:36.385880Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['text_model_v4'] = df_test['input_for_model'].progress_apply(generate_text,\n",
    "                                     model=v2_t5base_strict,\n",
    "                                     tokenizer=t5_base_tokenizer,\n",
    "                                     num_beams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T23:59:25.601529Z",
     "start_time": "2020-12-29T23:52:03.201662Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['text_model_v5'] = df_test['input_for_model'].progress_apply(generate_text,\n",
    "                                     model=v2_t5base_strict,\n",
    "                                     tokenizer=t5_base_tokenizer,\n",
    "                                     num_beams=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T00:07:55.567685Z",
     "start_time": "2020-12-29T23:59:25.602517Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['text_model_v6'] = df_test['input_for_model'].progress_apply(generate_text,\n",
    "                                     model=v6_t5base_relaxed,\n",
    "                                     tokenizer=t5_base_tokenizer,\n",
    "                                     num_beams=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T00:11:29.299332Z",
     "start_time": "2020-12-30T00:07:55.568856Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['text_model_v7'] = df_test['input_for_model'].progress_apply(generate_text,\n",
    "                                     model=v7_t5small_strict,\n",
    "                                     tokenizer=t5_small_tokenizer,\n",
    "                                     num_beams=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T00:15:10.160646Z",
     "start_time": "2020-12-30T00:11:29.300391Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['text_model_v8'] = df_test['input_for_model'].progress_apply(generate_text,\n",
    "                                     model=v8_t5small_relaxed,\n",
    "                                     tokenizer=t5_small_tokenizer,\n",
    "                                     num_beams=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T00:23:57.703288Z",
     "start_time": "2020-12-30T00:15:56.875027Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['text_model_v9'] = df_test['input_for_model'].progress_apply(generate_text,\n",
    "                                     model=v9_mt5small_strict,\n",
    "                                     tokenizer=mt5_small_tokenizer,\n",
    "                                     num_beams=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T00:35:20.650092Z",
     "start_time": "2020-12-30T00:23:57.704400Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['text_model_v10'] = df_test['input_for_model'].progress_apply(generate_text,\n",
    "                                     model=v10_mt5small_relaxed,\n",
    "                                     tokenizer=mt5_small_tokenizer,\n",
    "                                     num_beams=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correct some reference texts for evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:06:20.814994Z",
     "start_time": "2020-12-30T01:06:20.795515Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.loc[2, 'text_model_v11'] = 'Zhāng Huódīng (Baichen, Jilin, 1971), es una de las cantantes más famosas de ópera pekinesa.'\n",
    "df_test.loc[21, 'text_model_v11'] = 'Basilio Antonio Fergus Alexander conocido artísticamente como Basilio (Ciudad de Panamá, Panamá, 13 de octubre de 1947 - Miami, Estados Unidos, 11 de octubre de 2009) fue un cantante panameño.'\n",
    "df_test.loc[26, 'text_model_v11'] = 'Frankie Banali (Queens, 14 de noviembre de 1951) es un baterista estadounidense, reconocido por su trabajo con la banda de heavy metal multiplatino Quiet Riot, siendo el único miembro que queda de la formación clásica de la banda.'\n",
    "df_test.loc[28, 'text_model_v11'] = 'Vagif Mustafazade (en azerí: Vaqif Mustafazadə; 16 de marzo de 1940-16 de diciembre de 1979) fue un músico azerbaiyano de jazz, pianista y compositor.'\n",
    "df_test.loc[43, 'text_model_v11'] = 'Fernando Peña Soto (Lebrija, 1863-Utrera, circa 1930) más conocido como Popá Pinini o El Pinini, fue un cantaor flamenco.'\n",
    "df_test.loc[59, 'text_model_v11'] = 'Philip David Charles Collins, más conocido como Phil Collins (Chiswick, Middlesex, Inglaterra, 30 de enero de 1951), es un baterista, cantante, compositor, productor y actor británico, y uno de los artistas de mayor éxito de la música pop y soft rock.'\n",
    "df_test.loc[60, 'text_model_v11'] = 'Joey Belladonna (nacido como Joseph Bellardini el 13 de octubre de 1960 en Oswego, Nueva York) es un vocalista y baterista de heavy metal y thrash metal, más conocido por ser el cantante de la banda Anthrax.'\n",
    "df_test.loc[61, 'text_model_v11'] = 'Roy Alan Lynes (n. 25 de octubre de 1943, Redhill, Surrey, Inglaterra) es un músico y compositor inglés, conocido por haber sido teclista de la banda de rock Status Quo'\n",
    "df_test.loc[62, 'text_model_v11'] = 'Russell Freeman (Chicago, Illinois, 28 de mayo de 1926-Las Vegas, Nevada, 27 de junio de 2002) fue un pianista y compositor estadounidense de jazz.'\n",
    "df_test.loc[65, 'text_model_v11'] = 'Thea Garret (Tarxien, Malta; 15 de marzo de 1992) es una cantante maltesa.'\n",
    "df_test.loc[69, 'text_model_v11'] = '8 de abril de 1956, Dolores, provincia de Buenos Aires), apodado el \"Chacarero Cantor\", es un popular cantante folclórico argentino.'\n",
    "df_test.loc[72, 'text_model_v11'] = 'Sven Erik Kristiansen, más conocido como Maniac (Noruega, 4 de febrero de 1969), es el vocalista de la banda de black metal Skitliv. Maniac es conocido principalmente por haber sido el vocalista de la banda pionera del black metal noruego, Mayhem.'\n",
    "df_test.loc[73, 'text_model_v11'] = 'Kelly Beatrice Carrion Kipnis (n. 11 de marzo de 1996 en Rhode Island, Estados Unidos), también conocida como Kelly Carrion es una Cantante, compositora y Filántropa estadounidense'\n",
    "df_test.loc[78, 'text_model_v11'] = 'Sal Valentino (Salvatore Willard Spampinato, 8 de septiembre de 1942) es un cantante, compositor y productor discográfico estadounidense, reconocido por haber sido el cantante de la agrupación The Beau Brummels.'\n",
    "df_test.loc[80, 'text_model_v11'] = 'Sara Haydee Barreto Retuerto (Distrito de Sumbilca, Huaral, 28 de mayo de 1969-Lima, 28 de mayo de 2007), más conocida como Muñequita Sally, fue una cantante folclórica de huayno y huaylasrh reconocida en el Perú.'\n",
    "df_test.loc[81, 'text_model_v11'] = 'Leonardo Lozano Escalante, es un músico venezolano, concertista de cuatro venezolano, guitarra, compositor, arreglista y docente.'\n",
    "df_test.loc[84, 'text_model_v11'] = 'Cinthia Santibáñez (nacida el 13 de julio de 1973), músico, es la vocalista y principal letrista de la banda chilena Crisálida, que toma elementos del rock, el Metal y la música progresiva.'\n",
    "df_test.loc[85, 'text_model_v11'] = 'Daniel Paul Johns (Newcastle, 22 de abril de 1979) es un músico, vocalista, compositor, guitarrista y pianista australiano. Fue el líder de la banda de rock Silverchair.'\n",
    "df_test.loc[86, 'text_model_v11'] = 'Gordon Matthew Thomas Sumner, CBE (Wallsend, Tyneside del Norte, Inglaterra, 2 de octubre de 1951), más conocido como Sting, es un músico británico que se desempeñó inicialmente como bajista, y más tarde como cantante y bajista del grupo musical The Police, formando luego su propia banda.'\n",
    "df_test.loc[87, 'text_model_v11'] = 'Lee Min Hyuk (Seúl, 29 de noviembre de 1990), conocido como Minhyuk, es un cantante, rapero, actor y MC surcoreano. Es integrante de grupo masculino BtoB'\n",
    "df_test.loc[91, 'text_model_v11'] = 'Steve Walsh (n. 15 de junio de 1951) es un músico, cantante y compositor conocido principalmente por su trabajo como miembro de la banda estadounidense de rock progresivo Kansas.'\n",
    "df_test.loc[96, 'text_model_v11'] = 'Woodrow Wilson Guthrie (Okemah, Oklahoma, 14 de julio de 1912-Nueva York, 3 de octubre de 1967), conocido como Woody Guthrie, fue un músico y cantautor folk estadounidense.'\n",
    "df_test.loc[99, 'text_model_v11'] = 'Iñaki Antón González (Bilbao, 3 de agosto de 1964), conocido como simplemente Iñaki Antón o con el apodo de Uoho, es un músico multiinstrumentista, compositor y productor de rock español. Fue guitarrista de la bandas de rock Extremoduro.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For human evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-30T01:08:40.062924Z",
     "start_time": "2020-12-30T01:08:40.058210Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(df_test, open( \"../Evaluation/eval_data.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For automatic evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(file_name, text):\n",
    "    \n",
    "    file_name = '/home/hlaboa-server/jupyter/TFM/nlg_wikimusica/Evaluation/'+file_name+'.txt'\n",
    "    text_file = open(file_name, 'w')\n",
    "    n = text_file.write(text)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Elizabeth Mary Landreaxu, it's duplicates\n",
    "eval_data = eval_data.drop([70])\n",
    "\n",
    "# Take just 80 rows for evaluation\n",
    "eval_data = eval_data.iloc[:80]\n",
    "\n",
    "\n",
    "\n",
    "col_names = ['reference',\n",
    "              'text_model_v1',\n",
    "              'text_model_v2',\n",
    "              'text_model_v3',\n",
    "              'text_model_v4',\n",
    "              'text_model_v5',\n",
    "              'text_model_v12',\n",
    "              'text_model_v7',\n",
    "              'text_model_v8',\n",
    "              'text_model_v9',\n",
    "              'text_model_v10']\n",
    "\n",
    "file_names = ['references',\n",
    "              't5-base_vanilla_b0', \n",
    "              't5-base_strict_b0',\n",
    "              't5-base_strict_b2',\n",
    "              't5-base_strict_b5',\n",
    "              't5-base_strict_b10',\n",
    "              't5-base_relaxed_b10',\n",
    "              't5-small_strict_b10',\n",
    "              't5-small_relaxed_b10',\n",
    "              'mt5-small_strict_b10',\n",
    "              'mt5-small_relaxed_b10']\n",
    "\n",
    "\n",
    "for ind,col in enumerate(col_names):\n",
    "    text = '\\n'.join(eval_data[col].tolist())\n",
    "    write_file(file_names[ind], text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
